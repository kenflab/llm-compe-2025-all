#!/bin/bash
#SBATCH --partition=P07
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=64G
#SBATCH --time=02:00:00
#SBATCH --job-name=step3_msgs
#SBATCH --output=step3_msgs_%j.log
set -eE -o pipefail

# --- 環境 ---
module reset
module load nccl/2.22.3
module load hpcx/2.18.1-gcc-cuda12/hpcx-mt
module load miniconda/24.7.1-py311
source /home/appli/miniconda3/24.7.1-py311/etc/profile.d/conda.sh

# ログインノード負荷対策
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=1
export TORCH_COMPILE_DISABLE=1
export NVTE_FUSED_ATTN=0
export NCCL_IB_DISABLE=1

# 共有環境（フォールバックあり）
share_path="${SHARE_ENV:-/home/Competition2025/P07/shareP07/share_env}"
conda activate "$share_path/multi_sft_and_vllm" || { echo "[FATAL] conda activate failed"; exit 1; }

cd "$HOME"
echo "PY=$(which python)"; python -V
python - <<'PY'
try:
    import pyarrow as pa, pyarrow.parquet as pq
    print("pyarrow OK:", pa.__version__)
except Exception as e:
    print("[FATAL] pyarrow not available:", e)
    raise SystemExit(3)
PY

export PYTHONPATH="$share_path/deps:$PYTHONPATH"

# 認証 / W&B
export HUGGINGFACE_HUB_TOKEN="${HUGGINGFACE_HUB_TOKEN:-${HF_TOKEN:-}}"
case "${WANDB_MODE:-offline}" in
  online)   unset WANDB_DISABLED; export WANDB_MODE=online ;;
  offline)  unset WANDB_DISABLED; export WANDB_MODE=offline ;;
  disabled) export WANDB_DISABLED=true; unset WANDB_MODE ;;
esac

# 可変パラメータ
MODEL="${MODEL:-Qwen/Qwen3-8B}"
MBS="${MBS:-4}"
EPOCHS="${EPOCHS:-1}"
HEAD_N="${HEAD_N:-512}"
RESUME="${RESUME:-disable}"

# データ入出力
SRC="$HOME/data/gsm8k"             # Step2のParquetを置く場所（ローカル）
DST="$HOME/data_step3/gsm8k"        # Step3のmessages Parquet
mkdir -p "$DST"

# --- Step2.5: HFデータセットから直接取り込み（オプション） ---
HF_DSET="${HF_DSET:-}"              # 例: your-org/step25-gsm8k-msgs
HF_REVISION="${HF_REVISION:-}"      # 任意
TRAIN_SPLIT="${TRAIN_SPLIT:-train}" # 任意
VAL_SPLIT="${VAL_SPLIT:-test}"      # 任意

if [[ -n "$HF_DSET" ]]; then
  echo "[info] Importing from HF: $HF_DSET (rev=$HF_REVISION, splits=$TRAIN_SPLIT/$VAL_SPLIT)"
  python - <<'PY'
import os, pyarrow as pa, pyarrow.parquet as pq
from datasets import load_dataset

home=os.environ["HOME"]
dst=os.path.join(home,"data_step3","gsm8k")
os.makedirs(dst, exist_ok=True)
dname=os.environ["HF_DSET"]
rev=os.environ.get("HF_REVISION") or None
ts=os.environ.get("TRAIN_SPLIT","train")
vs=os.environ.get("VAL_SPLIT","test")

def to_msgs(rec):
    # すでに messages 列がある場合はそのまま
    if "messages" in rec:
        return {"messages": rec["messages"]}
    # question/answer から messages を作る場合
    q = rec.get("question") or rec.get("input") or rec.get("prompt")
    a = rec.get("answer") or rec.get("output") or rec.get("response")
    if q is None or a is None:
        return None
    return {"messages":[
        {"role":"user","content":q,"reasoning_content":""},
        {"role":"assistant","content":a,"reasoning_content":""},
    ]}

def save(split_name, out_name):
    ds = load_dataset(dname, split=split_name, revision=rev)
    out=[]
    for r in ds:
        m = to_msgs(r)
        if m: out.append(m)
    pq.write_table(pa.Table.from_pylist(out), os.path.join(dst, f"{out_name}.parquet"))

save(ts,"train")
save(vs,"test")
PY
fi

# --- ローカルStep2成果 → messages 変換（HF_DSET未指定時のフォールバック） ---
if [[ ! -s "$DST/train.parquet" || ! -s "$DST/test.parquet" ]]; then
  if [[ ! -s "$SRC/train.parquet" || ! -s "$SRC/test.parquet" ]]; then
    echo "[FATAL] ソースが無い: $SRC/{train,test}.parquet  または HF_DSET を指定してください"
    exit 2
  fi
  echo "[info] messages データを生成: $SRC → $DST"
  python - <<'PY'
import os, pyarrow as pa, pyarrow.parquet as pq
from pathlib import Path
home=os.environ["HOME"]
src=Path(home)/"data/gsm8k"
dst=Path(home)/"data_step3/gsm8k"; dst.mkdir(parents=True, exist_ok=True)
def convert(split):
    t=pq.read_table(src/f"{split}.parquet")
    out=[]
    for row in t.to_pylist():
        q = row["extra_info"]["question"]
        a = row["extra_info"]["answer"]
        final = a.split("####")[-1].strip()
        if not final.startswith("\\boxed"):
            final = f"\\boxed{{{final}}}"
        out.append({"messages":[
            {"role":"user","content":q,"reasoning_content":""},
            {"role":"assistant","content":final,"reasoning_content":a.replace("####","").strip()},
        ]})
    pq.write_table(pa.Table.from_pylist(out), dst/f"{split}.parquet")
convert("train"); convert("test")
PY
fi

# ヘッド抽出
TRAIN="$DST/train.parquet"; VAL="$DST/test.parquet"
if [[ "${HEAD_N:-0}" != "0" ]]; then
  python - <<PY
import pyarrow.parquet as pq, pathlib
base=pathlib.Path("$DST"); n=int("$HEAD_N")
for sp in ["train","test"]:
    t=pq.read_table(base/f"{sp}.parquet").slice(0,n)
    pq.write_table(t, base/f"{sp}.tiny.parquet")
PY
  TRAIN="$DST/train.tiny.parquet"; VAL="$DST/test.tiny.parquet"
fi

echo "TRAIN=$TRAIN"
echo "VAL=$VAL"
echo "MODEL=$MODEL  MBS=$MBS  EPOCHS=$EPOCHS"
echo "RESUME=$RESUME"

# 学習
PYTHONPATH="$share_path/deps" torchrun --standalone --nnodes=1 --nproc_per_node=1 \
  -m verl.trainer.fsdp_sft_trainer \
  data.train_files="$TRAIN" \
  data.val_files="$VAL" \
  data.multiturn.enable=true \
  data.multiturn.messages_key=messages \
  data.multiturn.tools_key=tools \
  data.multiturn.enable_thinking_key=enable_thinking \
  data.max_length=1024 \
  data.truncation=error \
  data.micro_batch_size_per_gpu="$MBS" \
  model.partial_pretrain="$MODEL" \
  model.fsdp_config.model_dtype=bfloat16 \
  trainer.n_gpus_per_node=1 \
  trainer.total_epochs="$EPOCHS" \
  trainer.default_local_dir="$HOME/training/sft/checkpoints" \
  trainer.logger='[console,wandb]' \
  trainer.project_name=sft_step3_messages \
  trainer.experiment_name="step3msgs_${MODEL##*/}_${SLURM_JOB_ID:-manual}" \
  trainer.resume_mode="$RESUME"
